### 1.scrapy的基本结构(五个部分是什么,请求发出去的整个流程)
>>>
    Scrapy Engine
    引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。
    
    调度器(Scheduler)
    调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。
    
    下载器(Downloader)
    下载器负责获取页面数据并提供给引擎，而后提供给spider。
    
    Spiders
    Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。
    
    Item Pipeline
    Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。
    
    下载器中间件(Downloader middlewares)
    下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 下载器中间件(Downloader Middleware) 。
    
    Spider中间件(Spider middlewares)
    Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 Spider中间件(Middleware) 。
    数据流(Data flow)
    Scrapy中的数据流由执行引擎控制，其过程如下:
    
    引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
    引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。
    引擎向调度器请求下一个要爬取的URL。
    调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
    一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
    引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。
    Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。
    引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。
    (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。
### 2.scrapy的去重原理(指纹去重到底是什么)
>>>
    去重指纹是sha1(method + url + body + header)
### 3.scrapy中间件有几种类
>>> 
    位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
    
    爬虫中间件(Spider Middlewares)介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
    
    调度中间件(Scheduler Middlewares)介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。
    
    下载中间件(Download Middlewares)介于Scrapy引擎和下载之间的中间件, 主要工作是将下载器生成的response发送给引擎.

### 4.scrapy中间件再哪里起作用
### 5.为什么要用到代理
### 6.代理怎么使用(具体代码,请求在什么时候添加的代理)
### 7.代理失效了怎么处理
### 8.登录验证码处理
### 9.爬取速度过快出现的验证码处理
### 10.如何用机器识别验证码
>>>
    1.摸清验证码验证走的网络请求流程

    2.对图片进行降噪处理。分离图片中的问题和候选文字
    
    3.对问题进行识别
    
    4.对候选文字进行切割并识别
    
    5.依次返回候选文字中点坐标
    
### 11.模拟登录流程
>>>
    因为http请求是无状态的，网站为了识别用户身份，
    需要通过cookie记录用户信息（用户、密码），
    这些信息都会在手动登陆时记录在post请求的form-data里，
    那么在爬虫时候只需要将这些信息添加到请求头里即可


### 12.cookie如何处理
### 13.如何处理网站传参加密的情况
### 14.分布式如何判断爬虫已经停止
### 15.分布式去重原理
>>>
    布隆过滤
    一个超大的位数组和几个哈希函数,
### 16.关系型数据库和非关系型数据库的区别
>>> 
    1.关系型数据库通过外键关联来建立表与表之间的关系，

    2.非关系型数据库通常指数据以对象的形式存储在数据库中，而对象之间的关系通过每个对象自身的属性来决定


### 17.爬下来数据你会选择什么存储方式,为什么
### 18.redis如何实现持久化
>>>
    1.snapshotting(快照)
    将内存中数据以快照的方式写到二进制文件中，默认的文件名称为dump.rdb
    
    2. Append-onlyfile(缩写aof)的方式   
    redis会将每一个收到的写命令都通过write函数追加到文件中
    
    3.bgRewriteAOF功能(Redis2.4)
    可以利用Redis的主从复制功能来实现性能和持久化的双赢局面。

### redis数据库有哪几种数据结构？
>>>
    String字符串

    List列表
    
    Set集合
    
    Hash散列
    
    Zset有序集合

    
### 19.mongodb是都支持事务
### 20.python2和python3的区别,如何实现python2代码迁移到python3环境
>>>

    基本语法变化:
    · 真正的除法
    · 声明非局部变量
    · 比较不同时只支持!=, 不再支持<>
    · raw_input() 更改为 input()
    · print 变成函数 print() 
    · 捕获异常语法的变化
    
    字符串的变化:
    · 字符串统一为 unicode 的str
    · bytes 到 str 要 decode(), 反之 encode()
    · 取消 unicode(), 用 str() 即可
    · 使用format()或字符串的同名函数替代 % 格式化
    
    迭代器相关的变化:
     · 字典没有has_key()了, 只能用in运算符
     · range()的行为现在是xrange()
     · zip(), map(), filter()不再返回列表而是迭代器, itertools 中 izip(), imap(), ifilter() 取消
    
    类,模块和包方面变化:
     · 引入当前目录下的包必须指定相对路径
     · urllib 和 urllib2 合并为 urllib
     · StringIO 可直接引入 io
    
    使用six
    
### 21.python2和python3的编码方式有什么差别
>>> 
    input()和raw_input()
    For循环变量和全局命名空间泄漏, 在 Python 3.x 中 for 循环变量不会再导致命名空间泄漏。
    Python 2 有 ASCII str() 类型，unicode() 是单独的，不是 byte 类型。现在， 在 Python 3，我们最终有了 Unicode (utf-8) 字符串，
    整除
    print函数,Python 2 的 print 声明已经被 print() 函数取代了
    Python 3.x 介绍的 一些Python 2 不兼容的关键字和特性可以通过在 Python 2 的内置 __future__ 模块导入

### 22.主要使用什么样的结构化数据提取方式
>>>
    xpath  正则  selector json等
    
### 23.动态加载的数据如何获取
>>>
    模拟ajax请求，返回json形式的数据
    
### 24.Celery - 分布式任务队列
>>> 
    专注于实时处理的任务队列，同时也支持任务调度。
    
### 25.__new__和__init__的区别
>>> 
    1.__new__是一个静态方法,而__init__是一个实例方法
    2.__new__方法会返回一个创建的实例,而__init__方法不会
    3.只有在__new__返回一个cls的实例后面的__init__才能被调用.
    4.当创建一个新实例时调用__new__,初始化一个实例时用__init__

### 26.实现方法A的装饰器timeout,5秒之后如果A还没有运行完毕,则抛出异常
>>> 
```python
import signal
import functools


class TimeoutError(Exception):
    pass


def timeout(seconds, error_message="Timeout Error: the cmd 30s have not finished."):
    def decorated(func):
        result = ""

        def _handle_timeout(signum, frame):
            global result
            result = error_message
            raise TimeoutError(error_message)

        def wrapper(*args, **kwargs):
            global result
            signal.signal(signal.SIGALRM, _handle_timeout)
            signal.alarm(seconds)

            try:
                result = func(*args, **kwargs)
            finally:
                signal.alarm(0)
                return result
            # return result

        return functools.wraps(func)(wrapper)

    return decorated


@timeout(3)  # 限定下面的slowfunc函数如果在5s内不返回就强制抛TimeoutError Exception结束
def slowfunc(sleep_time):
    a = 1
    import time
    time.sleep(sleep_time)
    return a


# slowfunc(3) #sleep 3秒，正常返回 没有异常


print(slowfunc(11)) # 被终止)

```
### 27.SSL/TLS的基本过程
>>> 
    SSL/TLS协议的基本思路是采用公钥加密法，

    （1） 客户端向服务器端索要并验证公钥。
    （2） 双方协商生成"对话密钥"。
    （3） 双方采用"对话密钥"进行加密通信。
### 28.简述网关,SNAT和DNAT分别有什么作用
>>>

    SNAT：源地址转换，是Linux防火墙的一种地址转换操作，也是iptables命令中的一种数据包控制类型，其作用是根据指定条件修改数据包的源IP地址。
    DNAT：目标地址转换，是Linux防火墙的另一种地址转换操作，同样也是iptables命令中的一种数据包控制类型，其作用是根据指定条件修改数据包的目标IP地址，目标端口。

### 29.正则提取a标签里的href属性
>>> 
    <a href=[\"|\']?(.*?)[\"|']
 
### 30.SVN和GIT的差别
>>>
    SVN属于集中化的版本控制系统，Git是一个分布式版本控制系统.
    在Git 中的绝大多数操作都只需要访问本地文件和资源，不必联网就可以看到所有的历史版本记录，而SVN 却需要联网。
    SVN 断开网络或者断开VPN就无法commit代码，但是Git 可以先commit到本地仓库。
    Git 的内容完整性要优于SVN。
    Git 克隆一个完整项目的速度非常快，SVN 非常慢。

### 31.单例模式
>>>
    单例模式是一种常见的软件设计模式.在它的核心结构中只包含一个被称为单例类的特殊类.
    通过单例模式可以保证系统中的一个类只有一个实例而且该实例易于外界访问,从而方便对实例个数的控制并节约系统资源.
    如果希望在系统中某一个类的对象只能存在一个,单例模式是最好的解决方式.
    
    4种方式实现单例模式
    - 使用模块
    - 使用 __new__
    ```python
    class Singleton(object):
        _instance = None
        def __new__(cls, *args, **kw):
            if not cls._instance:
                cls._instance = super(Singleton, cls).__new__(cls, *args, **kw)
                return cls._instance
    # 在上面的代码中，我们将类的实例和一个类变量 _instance 关联起来，如果 cls._instance 为 None 则创建实例，否则直接返回 cls._instance。
    ```
    - 使用装饰器（decorator）
    - 使用元类（metaclass）
    
### 32.Python垃圾回收机制
>>> 
    Python GC主要使用引用计数（reference counting）来跟踪和回收垃圾。
    优点:简单,实时性
    缺点:维护引用计数消耗资源,循环引用

    
### 33.装饰器
>>> 
    装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，
    较为经典的有插入日志、性能测试、事务处理等。
    装饰器的作用就是为已经存在的对象添加额外的功能。
    
### 34.MySQL索引原则
>>>
    1.搜索的索引列
    要建索引的列不一定是所要结果的列，简单的说就是where条件中出现的列需要索引，或者链接子句用到的列需要索引，而只在select中出现的列不需要索引。

    2.使用唯一索引
    考虑某列中值的分布，索引列的基数越大越好，例如存放出生日期的列具有不同的值，很容易区分各行;而存放性别的列，只有两个值，所以对这种列加索引也没有什么价值，不管搜哪个值，都是大约一半的数据量。
    
    3.使用短索引
    如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。例如，有一个char（200）的列，如果前10～20个字符内，多数值是唯一的，那么就不要对整个列进行索引（时刻考虑对资源的占用问题）。
    
    4.利用最左前缀
    在创建一个n列的索引时，实际是创建了mysql可利用的n个索引。多列索引可以起几个索引的作用，因为可以利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。
    
    5.不要过度索引
    什么列都建索引是错误的，比如从来都不查询的列，建上索引后一次也不用没什么价值，反而占用了额外的资源。

    索引的本质
    索引（Index）是帮助MySQL高效获取数据的数据结构。
    一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。
    索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。
    
### 35.三次握手
>>>
    1.客户端向服务器端发送一个SYN,随机数为A
    2.服务器收到正确的SYN之后,端向客户端送回SYN/ACK,ACK为A+1,SYN/ACK为随机数B
    3.客户端收到SYN/ACK之后,再发送一个ACK,当服务器端收到这个ACK之后就完成了三次握手,并进入链接状态.

### 36.抓包HTTPS的核心
>>>
    构造一个中间人代理，它有能力完成TLS/SSL握手
    弄到一个根证书，并用它生成签名认证的代理服务器证书

### 37.消息队列
>>>
    消息（Message）是指在应用间传送的数据。
    消息队列（Message Queue）是一种应用间的通信方式，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。
    
    RabbitMQ 特点
    RabbitMQ 是一个由 Erlang 语言开发的 AMQP 的开源实现。
    
    可靠性（Reliability）
    灵活的路由（Flexible Routing）
    消息集群（Clustering）
    高可用（Highly Available Queues）
    多种协议（Multi-protocol）
    多语言客户端（Many Clients）
    管理界面（Management UI）
    跟踪机制（Tracing）
    插件机制（Plugin System）
    
    RabbitMQ 基本概念
    Message
    消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。
    Publisher
    消息的生产者，也是一个向交换器发布消息的客户端应用程序。
    Exchange
    交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。
    Binding
    绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。
    Queue
    消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。
    Connection
    网络连接，比如一个TCP连接。
    Channel
    信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。
    Consumer
    消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。
    Virtual Host
    虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。
    Broker
    表示消息队列服务器实体。
    
 ### 38.python中的多线程
 >>>
    Python中存在有一个全局解析器锁，其限制是在同一时间内，Python解析器只能运行一个线程的代码。
    通过多进程+线程+异步IO来发挥我们CPU的最大性能。
### 39.HTTPS的工作原理
>>>
    1.浏览器将自己支持的一套加密规则发送给网站。 
    2.网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。 
    3.浏览器获得网站证书之后浏览器要做以下工作： 
    a) 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。 
    b) 如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。 
    c) 使用约定好的HASH算法计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。 
    4.网站接收浏览器发来的数据之后要做以下的操作： 
    a) 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。 
    b) 使用密码加密一段握手消息，发送给浏览器。 
    5.浏览器解密并计算握手消息的HASH ，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密。
    
### 40.破解JS加密的Cookie
>>>
    发现浏览器对这个页面加载了两次
    第一次返回一段Js代码,第二次返回正确内容
    首次请求数据时，服务端返回动态的混淆加密过的JS
    而这段JS的作用是给Cookie添加新的内容用于服务端验证
    那就是利用浏览器的JS代码调试功能
    PyV8,python可以直接与javascript操作

### 41.websocket
>>>
    WebSocket使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。
    浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。
    
    较HTTP协议的好处:
    · 较少的控制开销。
    · 更强的实时性。
    · 保持连接状态。
    · 更好的二进制支持。
    · 可以支持扩展。
    · 更好的压缩效果。
    
    websocket的应用场景:
    · 直播平台的弹幕
    · 实时聊天
    · 股票
    
    和http字段不一样的地方:
    · Connection必须设置Upgrade，表示客户端希望连接升级。
    · Upgrade字段必须设置Websocket，表示希望升级到Websocket协议。
    · Sec-WebSocket-Key是随机的字符串，服务器端会用这些数据来构造出一个SHA-1的信息摘要。把“Sec-WebSocket-Key”加上一个特殊字符串“258EAFA5-E914-47DA-95CA-C5AB0DC85B11”，然后计算SHA-1摘要，之后进行BASE-64编码，将结果做为“Sec-WebSocket-Accept”头的值，返回给客户端。如此操作，可以尽量避免普通HTTP请求被误认为Websocket协议。
    · Sec-WebSocket-Version 表示支持的Websocket版本。RFC6455要求使用的版本是13，之前草案的版本均应当弃用。
    · Origin字段是可选的，通常用来表示在浏览器中发起此Websocket连接所在的页面，类似于Referer。但是，与Referer不同的是，Origin只包含了协议和主机名称。
    · 其他一些定义在HTTP协议中的字段，如Cookie等，也可以在Websocket中使用。

    抓包:
    chrome里请求分类属于ws
    fiddle里找状态码是101的
    
    事件:
    · on_open:表示刚刚连接的时候
    · onmessage:表示收到消息怎么做
    · send:表示给服务器发送消息
    · on_close:表示关闭连接

###42.scrapy-redis
>>>
    scrapy-redis是为了更方便的实现scrapy分布式爬虫，而提供了一些以redis为基础的组件.
    因为redis支持主从同步，而且数据都是缓存在内存中
    对请求和数据的高频读取效率非常高
     
    主从同步:
    SLAVEOF命令或者设置slaveof选项，
    让一个服务器去复制（replicate）另一个服务器，
    我们称呼被复制的服务器为主服务器（master），
    而对主服务器进行复制的服务器则被称为从服务器（slave），
    当客户端向从服务器发送SLAVEOF命令，要求从服务器复制主服务器时，
    从服务器首先需要执行同步操作，也即是，将从服务器的数据库状态更新至主服务器当前所处的数据库状态

### 43.类变量和实例变量
>>>
    类变量：
	是可在类的所有实例之间共享的值.
	实例变量：
	实例化之后，每个实例单独拥有的变量。

### 44.Python自省
>>>
    运行时能够获得对象的类型.
    
### 45.闭包
>>>
    闭包(closure)是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。

    当一个内嵌函数引用其外部作作用域的变量,就会得到一个闭包.
    
### 46.MySQL的优化
>>>
    MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。
    MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引.
    
    不建议索引的情况:
    表记录很少(小于2000)
    索引的选择性较低(不重复的值与记录数的比值低,比如性别)
    
    永远使用一个与业务无关的自增字段作为主键。